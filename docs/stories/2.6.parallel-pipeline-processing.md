# Story 2.6: Parallel Pipeline Processing Implementation

## Status
Draft

## Story
**As a** system administrator managing high-volume Google News crawling operations,
**I want** the crawler to process URL resolution, article extraction, and database saving in parallel pipelines,
**so that** crawling throughput increases by 3x and articles are saved immediately as they're extracted instead of waiting for entire job completion.

## Acceptance Criteria
1. URL resolution, article extraction, and database saving run in parallel stages
2. Articles are saved to database immediately after extraction (not batch at job end)
3. Each stage processes items independently with queue-based communication
4. System achieves 3x throughput improvement over current sequential processing
5. Failed stages don't crash other stages (fault isolation)
6. Memory usage remains under 2GB per worker process
7. Real-time progress tracking shows articles saved during job execution
8. Queue backpressure prevents memory overflow when producer > consumer
9. Database connection pooling handles concurrent write operations
10. Pipeline supports configurable worker counts per stage (2-10 workers each)
11. Error recovery with dead letter queues for failed items
12. Comprehensive monitoring for queue depths and processing rates

## Tasks / Subtasks

### Phase 1: Core Pipeline Infrastructure (AC: 1,3,5)
- [ ] Create parallel pipeline service foundation (AC: 1,3)
  - [ ] Implement queue-based communication system between stages
  - [ ] Create worker pool management for each stage
  - [ ] Add stage isolation and fault tolerance mechanisms
- [ ] Design three-stage processing architecture (AC: 1)
  - [ ] Stage 1: URL Resolution (Playwright browsers)
  - [ ] Stage 2: Article Extraction (newspaper4k workers)
  - [ ] Stage 3: Database Saving (bulk operations)
- [ ] Implement queue management with size limits (AC: 8)
  - [ ] Add backpressure mechanisms to prevent memory overflow
  - [ ] Create queue monitoring and health checks
  - [ ] Implement dead letter queues for failed items

### Phase 2: Immediate Database Saving (AC: 2,7,9)
- [ ] Enhance SyncArticleRepository for concurrent operations (AC: 2,9)
  - [ ] Add `save_articles_batch()` method with conflict handling
  - [ ] Implement database connection pooling optimization
  - [ ] Add transaction management for concurrent writes
- [ ] Update job progress tracking system (AC: 7)
  - [ ] Add real-time progress reporting during execution
  - [ ] Implement `update_progress()` method in SyncCrawlJobRepository
  - [ ] Add database column `progress_percentage` to crawl_jobs table
- [ ] Integrate immediate saving into pipeline (AC: 2)
  - [ ] Connect extraction stage output to saving stage input
  - [ ] Implement error handling for save failures
  - [ ] Add progress updates after each successful batch save

### Phase 3: Performance Optimization (AC: 4,6,10)
- [ ] Implement configurable worker scaling (AC: 10)
  - [ ] Add configuration for workers per stage (URL: 2-3, Extract: 3-4, Save: 1-2)
  - [ ] Create worker pool auto-scaling based on queue depth
  - [ ] Add worker health monitoring and restart capabilities
- [ ] Optimize memory usage and resource management (AC: 6)
  - [ ] Implement memory cleanup after item processing
  - [ ] Add memory usage monitoring and alerting
  - [ ] Optimize Playwright browser instance management
- [ ] Add comprehensive performance monitoring (AC: 12)
  - [ ] Monitor queue depths and processing rates per stage
  - [ ] Track throughput metrics and memory usage
  - [ ] Add performance dashboards for pipeline health

### Phase 4: Integration and Testing (AC: 4,11)
- [ ] Modify SyncCrawlerEngine for parallel processing (AC: 4)
  - [ ] Replace `crawl_category_sync()` with `crawl_category_parallel()`
  - [ ] Integrate pipeline initialization and monitoring
  - [ ] Add fallback to sequential processing if pipeline fails
- [ ] Implement comprehensive error handling (AC: 11)
  - [ ] Add retry mechanisms with exponential backoff
  - [ ] Implement circuit breakers for stage failures
  - [ ] Create error aggregation and reporting
- [ ] Add pipeline testing and validation (AC: 4,5)
  - [ ] Create integration tests for concurrent processing
  - [ ] Add load testing with 1000+ URL batches
  - [ ] Validate 3x throughput improvement metrics

## Dev Notes

### Architecture Context
Based on current system architecture [Source: architecture/backend-architecture.md], the parallel pipeline will be implemented in the crawler engine layer (`src/core/crawler/`) with integration points to the task scheduling system (`src/core/scheduler/tasks.py`).

### Previous Story Insights
From Story 2.5 completion, the system now supports:
- Enhanced worker queue configuration with multiple queue types
- Increased URL processing limits (100 URLs vs previous 15)
- Improved browser tab management (20 tabs vs previous 10)
- Validated concurrent job execution (8-12 jobs simultaneously)

### Current Sequential Workflow Analysis
Current implementation in `src/core/crawler/sync_engine.py`:
1. Google News Search → URLs found
2. URL Resolution → Resolve all Google URLs to article URLs (batch of 10 URLs/browser)
3. Article Extraction → Extract all articles (batch of 5 threads)
4. Database Save → Save all articles at job completion

**Bottleneck**: Sequential processing where each stage waits for previous stage completion.

### Technical Implementation Details

**Queue Architecture Design:**
```python
# Pipeline queues for stage communication
url_queue = Queue(maxsize=1000)          # Raw Google URLs
resolved_queue = Queue(maxsize=1000)     # Resolved article URLs
extracted_queue = Queue(maxsize=500)     # Extracted article data
save_queue = Queue(maxsize=100)          # Ready for database
```

**Worker Configuration:**
- URL Resolvers: 2-3 Playwright browsers running concurrently
- Article Extractors: 3-4 newspaper4k worker threads
- Database Savers: 1-2 batch save workers with connection pooling

**File Locations for Implementation:**
- New pipeline service: `src/core/crawler/parallel_pipeline.py`
- Enhanced crawler: `src/core/crawler/sync_engine.py` (modify `crawl_category_sync()`)
- Repository enhancement: `src/database/repositories/sync_article_repo.py`
- Progress tracking: `src/database/repositories/sync_job_repo.py`
- Configuration: `src/shared/config.py` (add pipeline settings)

**Database Schema Updates:**
- Add `progress_percentage` DECIMAL column to `crawl_jobs` table
- Optimize article table indexes for concurrent writes
- Consider connection pool configuration in database settings

### Testing Standards

**Integration Testing Requirements:**
```python
# Test files location: tests/integration/
# Framework: pytest with testcontainers for real services

def test_parallel_pipeline_fault_isolation():
    """Test that one stage failure doesn't crash other stages"""

def test_concurrent_database_saves():
    """Test data integrity with multiple concurrent saves"""

def test_queue_backpressure_handling():
    """Test memory limits with queue size restrictions"""
```

**Performance Testing Requirements:**
- Load test with 1000+ URLs to validate 3x throughput improvement
- Memory usage monitoring under concurrent processing
- Database connection pool testing with concurrent writes
- Queue depth monitoring and backpressure validation

**Technology Stack:**
- Threading: ThreadPoolExecutor for URL resolution workers [Source: tech-stack.md]
- Async Processing: newspaper4k built-in threading for article extraction
- Database: PostgreSQL with connection pooling [Source: tech-stack.md]
- Queue Management: Python Queue with maxsize limits
- Monitoring: Python structlog for performance metrics [Source: tech-stack.md]

### Risk Mitigation Strategies

**High Risk Areas:**
1. **Concurrency Issues**: Implement thread-safe operations and comprehensive testing
2. **Memory Management**: Add queue size limits and cleanup procedures
3. **Database Contention**: Use connection pooling and transaction optimization

**Configuration Strategy:**
```yaml
# Environment variables for pipeline tuning
PIPELINE_URL_WORKERS=2
PIPELINE_EXTRACT_WORKERS=3
PIPELINE_SAVE_WORKERS=1
PIPELINE_QUEUE_SIZE=1000
PIPELINE_BATCH_SIZE=10
```

## Testing

### Testing Standards
**Test Framework:** pytest with testcontainers for integration tests requiring real services [Source: architecture/coding-standards.md]

**Test File Locations:**
- Unit tests: `tests/unit/test_core/test_crawler/test_parallel_pipeline.py`
- Integration tests: `tests/integration/test_parallel_processing.py`

**Required Test Coverage:**
- Queue management and backpressure handling
- Concurrent database operations with data integrity
- Stage fault isolation and error recovery
- Memory usage monitoring under load
- End-to-end pipeline performance validation

**Performance Testing:**
- Validate 3x throughput improvement vs sequential processing
- Memory usage <2GB per worker process
- Queue health monitoring with high throughput
- Database connection pool performance under concurrent load

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-09-29 | v1.0 | Initial story creation for parallel pipeline processing | BMad SM |

## Dev Agent Record

*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results

*This section will be populated by QA agent after implementation review*