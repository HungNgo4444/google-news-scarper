# Story 2.5: Critical Worker Queue and URL Processing Limits Fix

## Status
Ready for Implementation

## Story
**As a** system administrator managing the Google News Scraper infrastructure,
**I want** the Celery worker queue configuration and URL processing limits to be properly configured,
**so that** crawl jobs process efficiently with workers handling all queue types and processing 80-150 articles per job instead of the current 10-20 articles.

## Acceptance Criteria
1. Worker processes tasks from all queues (default, crawl_queue, maintenance_queue)
2. Jobs transition through states: PENDING → RUNNING → COMPLETED
3. Each successful job crawls minimum 80 articles (instead of current 10-20)
4. Queue health monitor reports "healthy" status
5. No stuck jobs in PENDING state for >10 minutes
6. 8-12 concurrent jobs running simultaneously
7. 70%+ article extraction success rate
8. <5 minutes average job execution time for 100 URLs
9. Redis queue length consistently near 0
10. Worker health checks pass consistently
11. Queue metrics show balanced load distribution
12. Error rate <10% for crawl tasks
13. Configuration validation prevents invalid settings

## Tasks / Subtasks

### Phase 1: Critical Queue Fix (P0)
- [ ] Update worker command in docker-compose.yml (AC: 1,2)
  - [ ] Add `--queues=default,crawl_queue,maintenance_queue` parameter to worker command
  - [ ] Update worker configuration in docker-compose.yml line 86
- [ ] Restart worker containers to apply new queue configuration (AC: 1,2)
  - [ ] Execute `docker-compose up -d worker` to restart with new config
  - [ ] Verify workers are listening to all queues using Celery inspect
- [ ] Verify jobs transition from PENDING to RUNNING (AC: 2,5)
  - [ ] Monitor job status transitions in Redis
  - [ ] Check queue health endpoint reports "healthy" status
- [ ] Monitor job execution for 1 hour (AC: 6,10,11)
  - [ ] Verify 8-12 concurrent jobs running simultaneously
  - [ ] Check worker health checks pass consistently

### Phase 2: URL Limits Enhancement (P1)
- [ ] Add environment variables to .env file (AC: 3,7,8)
  - [ ] Add MAX_URLS_TO_PROCESS=100 (increase from 15)
  - [ ] Add MAX_RESULTS_PER_SEARCH=200 (increase from 100)
  - [ ] Add MAX_TABS_PER_BROWSER=20 (increase from 10)
- [ ] Update settings configuration in config.py (AC: 13)
  - [ ] Add validation for MAX_URLS_TO_PROCESS range
  - [ ] Add validation for MAX_TABS_PER_BROWSER range
  - [ ] Add configuration validation in startup process
- [ ] Modify crawler limits in sync_engine.py and extractor.py (AC: 3,7,8)
  - [ ] Update sync_engine.py line 116: Change default from 15 to 100
  - [ ] Update extractor.py line 804: Make batch size configurable
  - [ ] Replace hard-coded limits with environment variable reads
- [ ] Test with sample category to verify increased URL processing (AC: 3,7,8)
  - [ ] Create test job and verify it processes minimum 80 articles
  - [ ] Verify average execution time <5 minutes for 100 URLs
  - [ ] Confirm 70%+ article extraction success rate

### Phase 3: Validation & Monitoring (P2)
- [ ] Create integration test for queue health (AC: 4,9,10)
  - [ ] Add test for queue health monitoring
  - [ ] Add test for Redis queue length validation
  - [ ] Add worker health check validation test
- [ ] Add configuration validation in startup (AC: 13)
  - [ ] Validate environment variables on startup
  - [ ] Add error handling for invalid configuration values
  - [ ] Create configuration validation utility function
- [ ] Enhanced monitoring for URL processing metrics (AC: 7,8,11,12)
  - [ ] Add metrics collection for articles per job
  - [ ] Add metrics for job execution time
  - [ ] Add error rate monitoring with <10% threshold
- [ ] Documentation updates for troubleshooting (AC: All)
  - [ ] Update troubleshooting guide with queue configuration
  - [ ] Document new environment variables and limits
  - [ ] Add monitoring and health check procedures

## Dev Notes

### Root Cause Analysis

**Issue 1: Worker Queue Configuration Mismatch**

Problem Location: `docker-compose.yml:86`
```bash
# Current worker command:
celery -A src.core.scheduler.celery_app worker --loglevel=debug --pool=threads --concurrency=2
```

Analysis:
- Worker only listens to `default` queue by default
- Crawl tasks are routed to `crawl_queue` (celery_app.py:84)
- Tasks are created but never processed → permanent PENDING status

**Issue 2: URL Processing Hard Limits**

Problem Locations:
1. **src/core/crawler/sync_engine.py:116-119**:
```python
MAX_URLS_TO_PROCESS = getattr(self.settings, 'MAX_URLS_TO_PROCESS', 15)  # ❌ Only 15 URLs
urls_to_process = google_news_urls[:MAX_URLS_TO_PROCESS]  # ❌ Truncates at 15
```

2. **src/core/crawler/extractor.py:804**:
```python
for i, url in enumerate(urls_batch[:10]):  # ❌ Only 10 URLs per batch
```

### Technical Context

**Relevant Source Tree:**
```
src/
├── core/
│   ├── scheduler/
│   │   ├── celery_app.py          # Task routing configuration (line 84)
│   │   └── tasks.py               # Task definitions
│   └── crawler/
│       ├── sync_engine.py         # URL processing limits (line 116)
│       └── extractor.py           # Batch processing (line 804)
├── shared/
│   └── config.py                  # Configuration settings
docker-compose.yml                 # Worker command configuration (line 86)
.env                              # Environment variables
```

**Architecture Dependencies:**
- **Docker Compose**: Multi-service architecture with PostgreSQL, Redis, and Celery
- **Redis**: Message broker for task queues (redis://redis:6379/0)
- **Celery**: Task routing with queues (default, crawl_queue, maintenance_queue)
- **Playwright**: Browser session management for URL processing

**Solution Design:**

**Approach A - Add Queue Specification (Recommended)**:
```bash
# Update docker-compose.yml worker command:
celery -A src.core.scheduler.celery_app worker --loglevel=debug --pool=threads --concurrency=2 --queues=default,crawl_queue,maintenance_queue
```

**Configuration Updates:**
```bash
# Add to .env:
MAX_URLS_TO_PROCESS=100        # From 15 → 100
MAX_RESULTS_PER_SEARCH=200     # From 100 → 200
MAX_TABS_PER_BROWSER=20        # From 10 → 20
```

### Testing

**Testing Standards:**
- Integration tests in `/tests/integration/`
- Use pytest framework for all tests
- Mock external dependencies (Redis, database) in unit tests
- Use testcontainers for integration tests requiring real services
- Test files should follow pattern: `test_{module_name}.py`

**Required Test Coverage:**
- Worker queue configuration validation
- URL processing limit enforcement
- End-to-end job execution flow
- Error handling and rollback procedures
- Configuration validation on startup

**Performance Testing:**
- Concurrent job execution (8-12 jobs)
- Large URL batch processing (100+ URLs)
- Memory usage monitoring under increased load
- Queue health and Redis performance

## 🚨 **Risk Assessment**

### High Risk Mitigation:
- **Worker Restart Required**: Schedule during low-traffic period
- **Configuration Changes**: Test on staging environment first
- **Backup Plan**: Revert docker-compose.yml if issues occur

### Medium Risk Monitoring:
- **Memory Usage**: Monitor worker memory with increased URL processing
- **Redis Load**: Monitor queue performance with higher throughput
- **Database Connections**: Ensure connection pool handles increased load

## ✅ **Acceptance Criteria**

### Functional Requirements:
- [ ] Worker processes tasks from all queues (default, crawl_queue, maintenance_queue)
- [ ] Jobs transition through states: PENDING → RUNNING → COMPLETED
- [ ] Each successful job crawls minimum 80 articles
- [ ] Queue health monitor reports "healthy" status
- [ ] No stuck jobs in PENDING state for >10 minutes

### Performance Requirements:
- [ ] 8-12 concurrent jobs running simultaneously
- [ ] 70%+ article extraction success rate
- [ ] <5 minutes average job execution time for 100 URLs
- [ ] Redis queue length consistently near 0

### Monitoring Requirements:
- [ ] Worker health checks pass consistently
- [ ] Queue metrics show balanced load distribution
- [ ] Error rate <10% for crawl tasks
- [ ] Configuration validation prevents invalid settings

## 🔄 **Rollback Plan**

If issues occur after deployment:

1. **Immediate Rollback**:
   ```bash
   # Revert docker-compose.yml worker command
   git checkout HEAD~1 docker-compose.yml
   docker-compose up -d worker
   ```

2. **Graceful Degradation**:
   - Reduce MAX_URLS_TO_PROCESS to 50 if memory issues
   - Switch to single queue routing if queue issues persist
   - Increase worker timeout if processing takes longer

3. **Emergency Stop**:
   ```bash
   # Stop all workers if critical issues
   docker-compose stop worker beat
   ```

## 📚 **Dependencies**

### Technical Dependencies:
- **Docker Compose**: Worker restart capability
- **Redis**: Queue management functionality
- **Celery**: Multi-queue routing support
- **Playwright**: Browser session management for increased URLs

### Business Dependencies:
- **Maintenance Window**: 15-30 minutes for worker restart
- **Staging Environment**: Testing before production deployment
- **Monitoring Setup**: Queue health and performance metrics

## 📝 **Testing Strategy**

### Unit Tests:
- [ ] Worker queue configuration validation
- [ ] URL processing limit enforcement
- [ ] Configuration parameter validation

### Integration Tests:
- [ ] End-to-end job execution (create → process → complete)
- [ ] Multi-queue task routing verification
- [ ] Batch URL processing with increased limits

### Performance Tests:
- [ ] Concurrent job execution (8-12 jobs)
- [ ] Large URL batch processing (100+ URLs)
- [ ] Memory usage under increased load

### Manual Testing:
- [ ] Create test job via API
- [ ] Monitor job progression through states
- [ ] Verify article extraction count meets targets
- [ ] Check queue health metrics

## 🚀 **Deployment Steps**

### Pre-Deployment:
1. **Backup current configuration**
2. **Test changes on staging environment**
3. **Prepare rollback procedures**
4. **Schedule maintenance window**

### Deployment:
1. **Update environment variables** (.env)
2. **Update worker configuration** (docker-compose.yml)
3. **Update code limits** (sync_engine.py, extractor.py)
4. **Restart worker services**
5. **Verify queue configuration**

### Post-Deployment:
1. **Monitor job execution** for 2 hours
2. **Verify performance metrics** meet targets
3. **Check error logs** for any issues
4. **Confirm increased article counts**

## 📖 **Related Documentation**

- **Architecture**: [Backend Architecture](../architecture/backend-architecture.md)
- **Deployment**: [Deployment Guide](../architecture/deployment.md)
- **Troubleshooting**: [TROUBLESHOOTING.md](../../TROUBLESHOOTING.md)
- **Previous Stories**:
  - [2.4 Critical Google News Extraction Fix](./2.4.critical-google-news-extraction-fix.md)
  - [2.3 Critical Crawling Engine Fix](./2.3.critical-crawling-engine-and-interface-fixes.md)

## 💡 **Future Enhancements**

### Short-term (Next Sprint):
- **Dynamic queue scaling** based on load
- **Intelligent URL filtering** to prioritize high-quality sources
- **Advanced batch size optimization** based on content type

### Long-term (Future Releases):
- **Auto-scaling worker instances** based on queue depth
- **Machine learning URL prioritization** for better success rates
- **Real-time queue rebalancing** for optimal performance

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2025-09-29 | 1.0 | Initial story creation with technical analysis | Claude Code |
| 2025-09-29 | 2.0 | Restructured to follow story template format | Product Owner Agent |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent_

### Debug Log References
_To be populated by dev agent_

### Completion Notes List
_To be populated by dev agent_

### File List
_To be populated by dev agent_

## QA Results
_Results from QA Agent review to be populated after implementation_

---

**Story Created**: 2025-09-29
**Last Updated**: 2025-09-29
**Status**: Ready for Implementation
**Estimated Effort**: 1-2 days
**Business Impact**: Critical - Directly affects core functionality